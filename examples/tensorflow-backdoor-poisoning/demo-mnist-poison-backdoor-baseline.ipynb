{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Backdoor Poisoning MNIST Demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo will cover a basic backdoor poisoning attack on an MNIST-LeNet model.\n",
    "The following two sections cover experiment setup and is similar across all demos.\n",
    "To access demo results in MlFlow, please follow the general experiment setup steps outlined in `basic-mlflow-demo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Experiment Name and MNIST Dataset\n",
    "\n",
    "Here we will import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected.\n",
    "\n",
    "**Important: Users will need to verify or update the following parameters:**\n",
    "\n",
    "- Ensure that the `USERNAME` parameter is set to your own name.\n",
    "- Ensure that the `DATASET_DIR` parameter is set to the location of the MNIST dataset directory. Currently set to `/nfs/data/Mnist` as the default location.\n",
    "- (Optional) Set the `EXPERIMENT_NAME` parameter to your own preferred experiment name.\n",
    "\n",
    "Other parameters can be modified to alter the RESTful API and MLFlow tracking addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Please enter custom username here.\n",
    "USERNAME = \"howard\"\n",
    "\n",
    "# Ensure that the dataset location is properly set here.\n",
    "DATASET_DIR = \"/nfs/data/Mnist\"\n",
    "\n",
    "# Experiment name (note the username_ prefix convention)\n",
    "EXPERIMENT_NAME = f\"{USERNAME}_mnist_backdoor_poison\"\n",
    "\n",
    "# Address for connecting the docker container to exposed ports on the host device\n",
    "HOST_DOCKER_INTERNAL = \"host.docker.internal\"\n",
    "# HOST_DOCKER_INTERNAL = \"172.17.0.1\"\n",
    "\n",
    "# Testbed API ports\n",
    "RESTAPI_PORT = \"30080\"\n",
    "MLFLOW_TRACKING_PORT = \"35000\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = (\n",
    "    f\"http://{HOST_DOCKER_INTERNAL}:{RESTAPI_PORT}\"\n",
    "    if os.getenv(\"IS_JUPYTER_SERVICE\")\n",
    "    else f\"http://localhost:{RESTAPI_PORT}\"\n",
    ")\n",
    "\n",
    "# Override the AI_RESTAPI_URI variable, used to connect to RESTful API service\n",
    "os.environ[\"AI_RESTAPI_URI\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = (\n",
    "    f\"http://{HOST_DOCKER_INTERNAL}:{MLFLOW_TRACKING_PORT}\"\n",
    "    if os.getenv(\"IS_JUPYTER_SERVICE\")\n",
    "    else f\"http://localhost:{MLFLOW_TRACKING_PORT}\"\n",
    ")\n",
    "\n",
    "# Override the MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Base API address\n",
    "RESTAPI_API_BASE = f\"{RESTAPI_ADDRESS}/api\"\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "import requests\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Import utils.py file\n",
    "import utils\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entrypoints that we will be running in this example are implemented in the Python source files under `src/` and the `MLproject` file.\n",
    "To run these entrypoints within the testbed architecture, we need to package those files up into an archive and submit it to the Testbed RESTful API to create a new job.\n",
    "For convenience, the `Makefile` provides a rule for creating the archive file for this example, just run `make workflows`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar czf workflows.tar.gz src/gaussian_augmentation.py src/infer.py src/defenses_image_preprocessing.py src/tasks.py src/attacks_poison_updated.py src/deploy_poison_data.py src/gen_poison_data.py src/train.py src/import_keras.py src/spatial_smoothing.py src/jpeg_compression.py src/init_model.py src/gen_poison_clean_data.py src/gen_poison_model.py src/registry_art_updated.py src/estimators_keras_classifiers_updated.py src/data_tensorflow_updated.py MLproject\n",
      "chmod 644 workflows.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `utils.py` file that is able to connect with the Testbed RESTful API using the HTTP protocol.\n",
    "We connect using the client below, which uses the environment variable `AI_RESTAPI_URI` to figure out how to connect to the Testbed RESTful API,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhuang/.conda/envs/tensorflow-mnist-classifier/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "restapi_client = utils.SecuringAIClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experimentId': 24,\n",
       " 'name': 'howard_mnist_backdoor_poison',\n",
       " 'lastModified': '2021-02-11T09:13:02.448572',\n",
       " 'createdOn': '2021-02-11T09:13:02.448572'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_experiment = restapi_client.get_experiment_by_name(name=EXPERIMENT_NAME)\n",
    "\n",
    "if response_experiment is None or \"Not Found\" in response_experiment.get(\"message\", []):\n",
    "    response_experiment = restapi_client.register_experiment(name=EXPERIMENT_NAME)\n",
    "\n",
    "response_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check which queues are available for running our jobs to make sure that the resources that we need are available.\n",
    "The code below queries the Testbed API and returns a list of active queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tensorflow_cpu',\n",
       "  'lastModified': '2020-11-20T17:46:06.756687',\n",
       "  'createdOn': '2020-11-20T17:46:06.756687',\n",
       "  'queueId': 1},\n",
       " {'name': 'tensorflow_gpu',\n",
       "  'lastModified': '2020-11-20T18:00:40.876888',\n",
       "  'createdOn': '2020-11-20T18:00:40.876888',\n",
       "  'queueId': 2},\n",
       " {'name': 'pytorch_cpu',\n",
       "  'lastModified': '2020-11-20T19:52:36.079781',\n",
       "  'createdOn': '2020-11-20T19:52:36.079781',\n",
       "  'queueId': 5},\n",
       " {'name': 'pytorch_gpu',\n",
       "  'lastModified': '2020-11-20T19:52:43.348460',\n",
       "  'createdOn': '2020-11-20T19:52:43.348460',\n",
       "  'queueId': 7}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restapi_client.list_queues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper functions will recheck the job responses until the job is completed or a run ID is available. \n",
    "The run ID is needed to link dependencies between jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_run_id_is_not_known(job_response):\n",
    "    return job_response[\"mlflowRunId\"] is None and job_response[\"status\"] not in [\n",
    "        \"failed\",\n",
    "        \"finished\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_run_id(job_response):\n",
    "    while mlflow_run_id_is_not_known(job_response):\n",
    "        time.sleep(1)\n",
    "        job_response = restapi_client.get_job_by_id(job_response[\"jobId\"])\n",
    "        \n",
    "    return job_response\n",
    "\n",
    "\n",
    "def wait_until_finished(job_response):\n",
    "    # First make sure job has started.\n",
    "    job_response = get_run_id(job_response)\n",
    "    \n",
    "    # Next re-check job until it has stopped running.\n",
    "    while (job_response[\"status\"] not in [\"failed\", \"finished\"]):\n",
    "        time.sleep(1)\n",
    "        job_response = restapi_client.get_job_by_id(job_response[\"jobId\"])\n",
    "    \n",
    "    return job_response\n",
    "\n",
    "\n",
    "# Helper function for viewing MLflow results.\n",
    "def get_mlflow_results(job_response):\n",
    "    mlflow_client = MlflowClient()\n",
    "    job_response = wait_until_finished(job_response)\n",
    "    \n",
    "    if(job_response['status']==\"failed\"):\n",
    "        return {}\n",
    "    \n",
    "    run = mlflow_client.get_run(job_response[\"mlflowRunId\"])  \n",
    "    \n",
    "    while(len(run.data.metrics) == 0):\n",
    "        time.sleep(1)\n",
    "        run = mlflow_client.get_run(job_response[\"mlflowRunId\"])\n",
    "        \n",
    "    return run\n",
    "\n",
    "\n",
    "def print_mlflow_results(response):\n",
    "    results = get_mlflow_results(response)\n",
    "    pprint.pprint(results.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Training: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to train our baseline model that will serve as a reference point for the effectiveness of our attacks.\n",
    "We will be submitting our job to the `\"tensorflow_gpu\"` queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job for LeNet-5 neural network submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T02:43:29.671351',\n",
      " 'dependsOn': None,\n",
      " 'entryPoint': 'train',\n",
      " 'entryPointKwargs': '-P batch_size=256 -P '\n",
      "                     'register_model_name=howard_mnist_backdoor_poison_le_net '\n",
      "                     '-P model_architecture=le_net -P epochs=30 -P '\n",
      "                     'data_dir_training=/nfs/data/Mnist/training -P '\n",
      "                     'data_dir_testing=/nfs/data/Mnist/testing',\n",
      " 'experimentId': 24,\n",
      " 'jobId': '4e08ad6d-da2f-4e6e-97a6-cf57d06c8a46',\n",
      " 'lastModified': '2021-05-13T02:43:29.671351',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '1h',\n",
      " 'workflowUri': 's3://workflow/b1968fc06b4447e2932b01816f432bf8/workflows.tar.gz'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d4f083b77ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mresponse_le_net_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_run_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_le_net_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint_mlflow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_le_net_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7376437e53ba>\u001b[0m in \u001b[0;36mprint_mlflow_results\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_mlflow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mlflow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7376437e53ba>\u001b[0m in \u001b[0;36mget_mlflow_results\u001b[0;34m(job_response)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mlflow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmlflow_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mjob_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait_until_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7376437e53ba>\u001b[0m in \u001b[0;36mwait_until_finished\u001b[0;34m(job_response)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Next re-check job until it has stopped running.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"finished\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mjob_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestapi_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_job_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"jobId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response_le_net_train = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "        \"-P batch_size=256\",\n",
    "        f\"-P register_model_name={EXPERIMENT_NAME}_le_net\",\n",
    "        \"-P model_architecture=le_net\",\n",
    "        \"-P epochs=30\",\n",
    "        f\"-P data_dir_training={DATASET_DIR}/training\",\n",
    "        f\"-P data_dir_testing={DATASET_DIR}/testing\",\n",
    "    ]),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    timeout=\"1h\",    \n",
    ")\n",
    "\n",
    "print(\"Training job for LeNet-5 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_train)\n",
    "\n",
    "response_le_net_train = get_run_id(response_le_net_train)\n",
    "print_mlflow_results(response_le_net_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Poisoned Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our set of poisoned images.\n",
    "Start by submitting the poison generation job below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar czf workflows.tar.gz src/gaussian_augmentation.py src/infer.py src/defenses_image_preprocessing.py src/tasks.py src/attacks_poison_updated.py src/deploy_poison_data.py src/gen_poison_data.py src/train.py src/import_keras.py src/spatial_smoothing.py src/jpeg_compression.py src/init_model.py src/gen_poison_clean_data.py src/gen_poison_model.py src/registry_art_updated.py src/estimators_keras_classifiers_updated.py src/data_tensorflow_updated.py MLproject\n",
      "chmod 644 workflows.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backdoor poison attack (LeNet-5 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T04:07:47.371157',\n",
      " 'dependsOn': '4e08ad6d-da2f-4e6e-97a6-cf57d06c8a46',\n",
      " 'entryPoint': 'gen_poison_data',\n",
      " 'entryPointKwargs': '-P data_dir=/nfs/data/Mnist/testing -P batch_size=100 -P '\n",
      "                     'target_class=1 -P poison_fraction=1 -P label_type=test',\n",
      " 'experimentId': 24,\n",
      " 'jobId': '13faaf6e-e02c-4d80-a908-2f10e7bf2432',\n",
      " 'lastModified': '2021-05-13T04:07:47.371157',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 1,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/78f754074238444aa4e1682de2b6f734/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create poisoned test images.\n",
    "response_gen_poison_le_net_test = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"gen_poison_data\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P data_dir={DATASET_DIR}/testing\",\n",
    "            \"-P batch_size=100\",\n",
    "            \"-P target_class=1\",\n",
    "            \"-P poison_fraction=1\",\n",
    "            \"-P label_type=test\"\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_train[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Backdoor poison attack (LeNet-5 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_gen_poison_le_net_test)\n",
    "print(\"\")\n",
    "\n",
    "response_gen_poison_le_net_test = get_run_id(response_gen_poison_le_net_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backdoor poison attack (LeNet-5 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T04:08:39.619121',\n",
      " 'dependsOn': '4e08ad6d-da2f-4e6e-97a6-cf57d06c8a46',\n",
      " 'entryPoint': 'gen_poison_data',\n",
      " 'entryPointKwargs': '-P data_dir=/nfs/data/Mnist/training -P batch_size=100 '\n",
      "                     '-P target_class=1 -P poison_fraction=0.1 -P '\n",
      "                     'label_type=train',\n",
      " 'experimentId': 24,\n",
      " 'jobId': 'a1b57486-b789-4dfc-ad50-a8f3348a9e77',\n",
      " 'lastModified': '2021-05-13T04:08:39.619121',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/1364d0a81f4b44e78a60ffb550753dd4/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create poisoned training images.\n",
    "response_gen_poison_le_net_train = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"gen_poison_data\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P data_dir={DATASET_DIR}/training\",\n",
    "            \"-P batch_size=100\",\n",
    "            \"-P target_class=1\",\n",
    "            \"-P poison_fraction=0.1\",\n",
    "            \"-P label_type=train\"\n",
    "            \n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_train[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Backdoor poison attack (LeNet-5 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_gen_poison_le_net_train)\n",
    "print(\"\")\n",
    "\n",
    "response_gen_poison_le_net_train = get_run_id(response_gen_poison_le_net_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job for LeNet-5 neural network submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T04:17:51.266625',\n",
      " 'dependsOn': 'a1b57486-b789-4dfc-ad50-a8f3348a9e77',\n",
      " 'entryPoint': 'train',\n",
      " 'entryPointKwargs': '-P batch_size=256 -P '\n",
      "                     'register_model_name=howard_mnist_backdoor_poison_data_poison_le_net '\n",
      "                     '-P model_architecture=le_net -P epochs=30 -P '\n",
      "                     'data_dir_training=/nfs/data/Mnist/training -P '\n",
      "                     'data_dir_testing=/nfs/data/Mnist/testing -P '\n",
      "                     'load_dataset_from_mlruns=true -P '\n",
      "                     'dataset_run_id_training=d8f3f95f589345b58eef79bcfcd3006a '\n",
      "                     '-P adv_tar_name=adversarial_poison.tar.gz -P '\n",
      "                     'adv_data_dir=adv_poison_data',\n",
      " 'experimentId': 24,\n",
      " 'jobId': '15380ce5-a617-4d7d-9704-b2ca351a4920',\n",
      " 'lastModified': '2021-05-13T04:17:51.266625',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '1h',\n",
      " 'workflowUri': 's3://workflow/4745878e05dd48e89190c0474bea66bc/workflows.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "# Train new model on poisoned training images\n",
    "response_le_net_poison_model = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "        \"-P batch_size=256\",\n",
    "        f\"-P register_model_name={EXPERIMENT_NAME}_data_poison_le_net\",\n",
    "        \"-P model_architecture=le_net\",\n",
    "        \"-P epochs=30\",\n",
    "        f\"-P data_dir_training={DATASET_DIR}/training\",\n",
    "        f\"-P data_dir_testing={DATASET_DIR}/testing\",\n",
    "        \"-P load_dataset_from_mlruns=true\",\n",
    "        f\"-P dataset_run_id_training={response_gen_poison_le_net_train['mlflowRunId']}\",\n",
    "        \"-P adv_tar_name=adversarial_poison.tar.gz\",\n",
    "        \"-P adv_data_dir=adv_poison_data\"\n",
    "    ]),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_gen_poison_le_net_train [\"jobId\"],\n",
    "    timeout=\"1h\",\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"Training job for LeNet-5 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_poison_model)\n",
    "\n",
    "response_le_net_poison_model = get_run_id(response_le_net_poison_model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference job for LeNet-5 neural network submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T04:46:37.785467',\n",
      " 'dependsOn': 'a1b57486-b789-4dfc-ad50-a8f3348a9e77',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=d785edaa864e414986bc436ac23bcde1 -P '\n",
      "                     'model_name=howard_mnist_backdoor_poison_le_net -P '\n",
      "                     'model_version=none -P batch_size=512 -P '\n",
      "                     'adv_tar_name=adversarial_poison.tar.gz -P '\n",
      "                     'adv_data_dir=adv_poison_data',\n",
      " 'experimentId': 24,\n",
      " 'jobId': '0e6bb254-ecf8-4817-8b3e-b311cc730857',\n",
      " 'lastModified': '2021-05-13T04:46:37.785467',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/b07e930e00204f49ac8e828df2c8cfeb/workflows.tar.gz'}\n",
      "{'accuracy': 0.9894999861717224,\n",
      " 'auc': 0.9995277523994446,\n",
      " 'loss': 0.03192389262840152,\n",
      " 'precision': 0.9907824993133545,\n",
      " 'recall': 0.9889000058174133}\n"
     ]
    }
   ],
   "source": [
    "# Inference: Regular model on poisoned test images.\n",
    "response_infer_reg_model = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [            \n",
    "            f\"-P run_id={response_gen_poison_le_net_test['mlflowRunId']}\",\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_le_net\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P batch_size=512\",\n",
    "            \"-P adv_tar_name=adversarial_poison.tar.gz\",\n",
    "            \"-P adv_data_dir=adv_poison_data\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_gen_poison_le_net_train[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Inference job for LeNet-5 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_infer_reg_model)\n",
    "\n",
    "print_mlflow_results(response_infer_reg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference job for LeNet-5 neural network submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T04:48:24.458782',\n",
      " 'dependsOn': '15380ce5-a617-4d7d-9704-b2ca351a4920',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=d785edaa864e414986bc436ac23bcde1 -P '\n",
      "                     'model_name=howard_mnist_backdoor_poison_data_poison_le_net '\n",
      "                     '-P model_version=none -P batch_size=512 -P '\n",
      "                     'adv_tar_name=adversarial_poison.tar.gz -P '\n",
      "                     'adv_data_dir=adv_poison_data',\n",
      " 'experimentId': 24,\n",
      " 'jobId': 'bdb8c7c1-534b-4fab-9a3c-89f048a15125',\n",
      " 'lastModified': '2021-05-13T04:48:24.458782',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/30b593f227d14e23a27c9d468558dee1/workflows.tar.gz'}\n",
      "{'accuracy': 0.11349999904632568,\n",
      " 'auc': 0.5075914263725281,\n",
      " 'loss': 20.509925842285156,\n",
      " 'precision': 0.11349999904632568,\n",
      " 'recall': 0.11349999904632568}\n"
     ]
    }
   ],
   "source": [
    "# Inference: Poisoned model on poisoned test images.\n",
    "response_infer_pos_model = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [            \n",
    "            f\"-P run_id={response_gen_poison_le_net_test['mlflowRunId']}\",\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_data_poison_le_net\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P batch_size=512\",\n",
    "            \"-P adv_tar_name=adversarial_poison.tar.gz\",\n",
    "            \"-P adv_data_dir=adv_poison_data\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_poison_model[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Inference job for LeNet-5 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_infer_pos_model)\n",
    "\n",
    "print_mlflow_results(response_infer_pos_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defending against the adversarial backdoor poisoning attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore available defenses on the adversarial backdoor poisoning attack.\n",
    "The following three jobs will run a selected defense (spatial smoothing, gaussian augmentation, or jpeg compression) and evaluate the defense on the baseline and backdoor trained models.\n",
    "\n",
    "- The first job uses the selected defense entrypoint to apply a preprocessing defense over the poisoned test images.\n",
    "- The second job runs the defended images against the poisoned backdoor model.\n",
    "- The final job runs the defended images against the baseline model.\n",
    "\n",
    "Ideally the defense will not impact the baseline model accuracy, while improving the backdoor model accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatial_smoothing defense (LeNet architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T05:39:15.312238',\n",
      " 'dependsOn': '13faaf6e-e02c-4d80-a908-2f10e7bf2432',\n",
      " 'entryPoint': 'spatial_smoothing',\n",
      " 'entryPointKwargs': '-P data_dir=/nfs/data/Mnist/testing -P batch_size=20 -P '\n",
      "                     'load_dataset_from_mlruns=true -P '\n",
      "                     'dataset_run_id=d785edaa864e414986bc436ac23bcde1 -P '\n",
      "                     'dataset_tar_name=adversarial_poison.tar.gz -P '\n",
      "                     'dataset_name=adv_poison_data',\n",
      " 'experimentId': 24,\n",
      " 'jobId': '80c0f0ca-171b-45ba-918b-0ae4f2e7488e',\n",
      " 'lastModified': '2021-05-13T05:39:15.312238',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/6c504e8643d540b2812a330b11027cbe/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "defenses = [\"gaussian_augmentation\", \"spatial_smoothing\", \"jpeg_compression\"]\n",
    "defense = defenses[1]\n",
    "\n",
    "response_poison_def = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=defense,\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P data_dir={DATASET_DIR}/testing\",\n",
    "            \"-P batch_size=20\",\n",
    "            \"-P load_dataset_from_mlruns=true\",\n",
    "            f\"-P dataset_run_id={response_gen_poison_le_net_test['mlflowRunId']}\",\n",
    "            \"-P dataset_tar_name=adversarial_poison.tar.gz\",\n",
    "            \"-P dataset_name=adv_poison_data\",\n",
    "            \n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_gen_poison_le_net_test[\"jobId\"],\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{defense} defense (LeNet architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_poison_def)\n",
    "print(\"\")\n",
    "\n",
    "response_poison_def = get_run_id(response_poison_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference job for LeNet-5 neural network submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T05:40:52.000626',\n",
      " 'dependsOn': '80c0f0ca-171b-45ba-918b-0ae4f2e7488e',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=c705e3cae2bd4fdaa064e825aa96e93e -P '\n",
      "                     'model_name=howard_mnist_backdoor_poison_data_poison_le_net '\n",
      "                     '-P model_version=none -P batch_size=512 -P '\n",
      "                     'adv_tar_name=spatial_smoothing_dataset.tar.gz -P '\n",
      "                     'adv_data_dir=adv_testing',\n",
      " 'experimentId': 24,\n",
      " 'jobId': '5d124fb2-0bfb-432f-a50a-fa5fbeefed78',\n",
      " 'lastModified': '2021-05-13T05:40:52.000626',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/46024827d7864404982ca04f152f7650/workflows.tar.gz'}\n",
      "{'accuracy': 0.9829000234603882,\n",
      " 'auc': 0.9996635913848877,\n",
      " 'loss': 0.05357005931437016,\n",
      " 'precision': 0.9861041307449341,\n",
      " 'recall': 0.9793000221252441}\n"
     ]
    }
   ],
   "source": [
    "# Inference: Poisoned model on poisoned test images.\n",
    "response_infer_pos_model = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_poison_def['mlflowRunId']}\",\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_data_poison_le_net\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P batch_size=512\",\n",
    "            f\"-P adv_tar_name={defense}_dataset.tar.gz\",\n",
    "            \"-P adv_data_dir=adv_testing\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_poison_def[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Inference job for LeNet-5 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_infer_pos_model)\n",
    "print_mlflow_results(response_infer_pos_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference job for LeNet-5 neural network submitted\n",
      "\n",
      "{'createdOn': '2021-05-13T05:42:36.064482',\n",
      " 'dependsOn': '80c0f0ca-171b-45ba-918b-0ae4f2e7488e',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=c705e3cae2bd4fdaa064e825aa96e93e -P '\n",
      "                     'model_name=howard_mnist_backdoor_poison_le_net -P '\n",
      "                     'model_version=none -P batch_size=512 -P '\n",
      "                     'adv_tar_name=spatial_smoothing_dataset.tar.gz -P '\n",
      "                     'adv_data_dir=adv_testing',\n",
      " 'experimentId': 24,\n",
      " 'jobId': '7164245f-3dea-4170-a895-6d8f7aed854f',\n",
      " 'lastModified': '2021-05-13T05:42:36.064482',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/48bcd354bf0f446ba45310f46d76ce7e/workflows.tar.gz'}\n",
      "{'accuracy': 0.9846000075340271,\n",
      " 'auc': 0.9994721412658691,\n",
      " 'loss': 0.046187656186521056,\n",
      " 'precision': 0.9869372844696045,\n",
      " 'recall': 0.982200026512146}\n"
     ]
    }
   ],
   "source": [
    "# Inference: Regular model on poisoned test images.\n",
    "response_infer_reg_model = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_poison_def['mlflowRunId']}\",\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_le_net\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P batch_size=512\",\n",
    "            f\"-P adv_tar_name={defense}_dataset.tar.gz\",\n",
    "            \"-P adv_data_dir=adv_testing\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_poison_def[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Inference job for LeNet-5 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_infer_reg_model)\n",
    "print_mlflow_results(response_infer_reg_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='querying_cell'></a>\n",
    "## Querying the MLFlow Tracking Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the lab API can only be used to register experiments and start jobs, so if users wish to extract their results programmatically, they can use the `MlflowClient()` class from the `mlflow` Python package to connect and query their results.\n",
    "Since we captured the run ids generated by MLFlow, we can easily retrieve the data logged about one of our jobs and inspect the results.\n",
    "To start the client, we simply need to run,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client uses the environment variable `MLFLOW_TRACKING_URI` to figure out how to connect to the MLFlow Tracking Service, which we configured near the top of this notebook.\n",
    "To query the results of one of our runs, we just need to pass the run id to the client's `get_run()` method.\n",
    "As an example, let's query the run results for the patch attack applied to the LeNet-5 architecture,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_le_net = mlflow_client.get_run(response_le_net_train[\"mlflowRunId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the request completed successfully, we should now be able to query data collected during the run.\n",
    "For example, to review the collected metrics, we just use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_le_net.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the run's parameters, we use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_le_net.data.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the run's tags, we use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_le_net.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things you can query using the MLFlow client.\n",
    "[The MLFlow documentation gives a full overview of the methods that are available](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
