{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST-LeNet Patch Demo\n",
    "\n",
    "This notebook demonstrates the adversarial patch attack applied on the LeNet model, as well as preprocessing and adversarial training defenses.\n",
    "\n",
    "The following two sections cover experiment setup and is similar across all demos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Experiment Name and MNIST Dataset\n",
    "\n",
    "Here we will import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected.\n",
    "\n",
    "### Important: Users will need to verify or update the following parameters:\n",
    "- Ensure that the `USERNAME` parameter is set to your own name.\n",
    "- Ensure that the `DATASET_DIR` parameter is set to the location of the MNIST dataset directory. Currently set to `/nfs/data/Mnist` as the default location.\n",
    "- (Optional) Set the `EXPERIMENT_NAME` parameter to your own preferred experiment name.\n",
    "\n",
    "Other parameters can be modified to alter the RESTful API and MLFlow tracking addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Please enter custom username here.\n",
    "USERNAME = \"howard\"\n",
    "\n",
    "# Experiment name (note the username_ prefix convention)\n",
    "EXPERIMENT_NAME = f\"{USERNAME}_mnist_adversarial_patches\"\n",
    "\n",
    "# Ensure that the MNIST dataset location is properly set here.\n",
    "DATASET_DIR = \"/nfs/data/Mnist\"\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:30080\"\n",
    "\n",
    "# Base API address\n",
    "RESTAPI_API_BASE = f\"{RESTAPI_ADDRESS}/api\"\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\"\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "import requests\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Import utils.py file\n",
    "import utils\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the Makefile works in your environment by executing the `bash` code block below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAvailable rules:\u001b[m\n",
      "\n",
      "\u001b[36mclean              \u001b[m Remove temporary files \n",
      "\u001b[36mdata               \u001b[m Download and prepare MNIST dataset \n",
      "\u001b[36minitdb             \u001b[m Initialize the RESTful API database \n",
      "\u001b[36mservices           \u001b[m Launch the Minio S3 and MLFlow Tracking services \n",
      "\u001b[36mteardown           \u001b[m Destroy service containers \n",
      "\u001b[36mworkflows          \u001b[m Create workflows tarball \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Running this will list the available rules defined in the demo's Makefile.\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset Overview:\n",
    "The training and testing images in this directory are saved as PNG files and are organized into the following folder structure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mnist\n",
    "    ├── testing\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   ├── 4\n",
    "    │   ├── 5\n",
    "    │   ├── 6\n",
    "    │   ├── 7\n",
    "    │   ├── 8\n",
    "    │   └── 9\n",
    "    └── training\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── 4\n",
    "        ├── 5\n",
    "        ├── 6\n",
    "        ├── 7\n",
    "        ├── 8\n",
    "        └── 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subfolders under `Mnist/training/` and `Mnist/testing/` are the classification labels for the images in the dataset.\n",
    "\n",
    "This folder structure is a standardized way to encode the label information and many libraries can make use of it, including the Tensorflow library that we are using for this particular demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: REST API and Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jobs that we will be running are implemented in the Python source files under `src/`, which will be executed using the entrypoints defined in the `MLproject` file.\n",
    "To get this information into the architecture, we need to package those files up into an archive and upload it to the lab API.\n",
    "For convenience, the `Makefile` provides a rule for creating the archive file, just run `make workflows`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'workflows'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `utils.py` file that is able to connect with the lab's RESTful API using the HTTP protocol.\n",
    "We connect using the client below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "restapi_client = utils.SecuringAIClient(address=RESTAPI_API_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'createdOn': '2020-11-05T08:45:14.946823',\n",
       " 'experimentId': 10,\n",
       " 'lastModified': '2020-11-05T08:45:14.946823',\n",
       " 'name': 'howard_mnist_adversarial_patches'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_experiment = restapi_client.get_experiment_by_name(name=EXPERIMENT_NAME)\n",
    "\n",
    "if response_experiment is None or \"Not Found\" in response_experiment.get(\"message\", []):\n",
    "    response_experiment = restapi_client.register_experiment(name=EXPERIMENT_NAME)\n",
    "\n",
    "response_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following sections will now cover the adversarial patch demo.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Patches: Baseline MNIST Training\n",
    "\n",
    "Now, we will train our baseline LeNet-5 model on the MNIST dataset. \n",
    "We will be submitting our jobs to the `\"tensorflow_gpu\"` queue.\n",
    "\n",
    "Once the experiment is finished, we will examine the accuracy results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job for LeNet-5 neural network submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T00:39:25.356043',\n",
      " 'dependsOn': None,\n",
      " 'entryPoint': 'train',\n",
      " 'entryPointKwargs': '-P batch_size=256 -P register_model=True -P '\n",
      "                     'model_architecture=le_net -P epochs=30 -P '\n",
      "                     'data_dir_train=/nfs/data/Mnist/training -P '\n",
      "                     'data_dir_test=/nfs/data/Mnist/testing',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'e6df0b0d-ec6b-4087-a7b1-4ab97064cfc6',\n",
      " 'lastModified': '2021-02-11T00:39:25.356043',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '1h',\n",
      " 'workflowUri': 's3://workflow/b8f949bd5661440b877e12683807557f/workflows.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "# Create and submit training job.\n",
    "\n",
    "response_le_net_train = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "        \"-P batch_size=256\",\n",
    "        \"-P register_model=True\",\n",
    "        \"-P model_architecture=le_net\",\n",
    "        \"-P epochs=30\",\n",
    "        f\"-P data_dir_train={DATASET_DIR}/training\",\n",
    "        f\"-P data_dir_test={DATASET_DIR}/testing\",\n",
    "    ]),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    timeout=\"1h\",\n",
    ")\n",
    "\n",
    "print(\"Training job for LeNet-5 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following helper functions will recheck the job responses until the job is completed or a run ID is available. \n",
    "# The run ID is needed to link dependencies between jobs.\n",
    "\n",
    "def mlflow_run_id_is_not_known(job_response):\n",
    "    return job_response[\"mlflowRunId\"] is None and job_response[\"status\"] not in [\n",
    "        \"failed\",\n",
    "        \"finished\",\n",
    "    ]\n",
    "\n",
    "def get_run_id(job_response):\n",
    "    while mlflow_run_id_is_not_known(job_response):\n",
    "        time.sleep(1)\n",
    "        job_response = restapi_client.get_job_by_id(job_response[\"jobId\"])\n",
    "        \n",
    "    return job_response\n",
    "\n",
    "def wait_until_finished(job_response):\n",
    "    # First make sure job has started.\n",
    "    job_response = get_run_id(job_response)\n",
    "    \n",
    "    # Next re-check job until it has stopped running.\n",
    "    while (job_response[\"status\"] not in [\"failed\", \"finished\"]):\n",
    "        time.sleep(1)\n",
    "        job_response = restapi_client.get_job_by_id(job_response[\"jobId\"])\n",
    "    \n",
    "    return job_response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job for LeNet-5 neural network\n",
      "{'createdOn': '2021-02-11T00:39:25.356043',\n",
      " 'dependsOn': None,\n",
      " 'entryPoint': 'train',\n",
      " 'entryPointKwargs': '-P batch_size=256 -P register_model=True -P '\n",
      "                     'model_architecture=le_net -P epochs=30 -P '\n",
      "                     'data_dir_train=/nfs/data/Mnist/training -P '\n",
      "                     'data_dir_test=/nfs/data/Mnist/testing',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'e6df0b0d-ec6b-4087-a7b1-4ab97064cfc6',\n",
      " 'lastModified': '2021-02-11T00:39:28.968235',\n",
      " 'mlflowRunId': 'fdc1fbd940bf4f2794a6d1c6b789d5b2',\n",
      " 'queueId': 2,\n",
      " 'status': 'started',\n",
      " 'timeout': '1h',\n",
      " 'workflowUri': 's3://workflow/b8f949bd5661440b877e12683807557f/workflows.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "# Now wait for the job to complete before proceeding to next steps.\n",
    "response_le_net_train = wait_until_finished(response_le_net_train)\n",
    "print(\"Training job for LeNet-5 neural network\")\n",
    "pprint.pprint(response_le_net_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking baseline MNIST job accuracy.\n",
    "\n",
    "Once the job has finished running we can view the results either through the MLflow URI or by accessing the job via MLflow client.\n",
    "\n",
    "Here we will show the baseline accuracy results from the previous training job.\n",
    "\n",
    "Please see [Querying the MLFlow Tracking Service](#querying_cell) section for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9820223450660706,\n",
      " 'auc': 0.9992511868476868,\n",
      " 'fn': 975.0,\n",
      " 'fp': 752.0,\n",
      " 'loss': 0.05927685966999091,\n",
      " 'precision': 0.9842615127563477,\n",
      " 'recall': 0.9796891808509827,\n",
      " 'restored_epoch': 3.0,\n",
      " 'stopped_epoch': 8.0,\n",
      " 'tn': 431284.0,\n",
      " 'tp': 47029.0,\n",
      " 'training_time_in_minutes': 5.73557385,\n",
      " 'val_accuracy': 0.9863287806510925,\n",
      " 'val_auc': 0.9989493489265442,\n",
      " 'val_fn': 175.0,\n",
      " 'val_fp': 147.0,\n",
      " 'val_loss': 0.046896340751505276,\n",
      " 'val_precision': 0.9877172708511353,\n",
      " 'val_recall': 0.9854118227958679,\n",
      " 'val_tn': 107817.0,\n",
      " 'val_tp': 11821.0}\n"
     ]
    }
   ],
   "source": [
    "def get_mlflow_results(job_response):\n",
    "    mlflow_client = MlflowClient()\n",
    "    job_response = wait_until_finished(job_response)\n",
    "    if(job_response['status']==\"failed\"):\n",
    "        return {}\n",
    "    \n",
    "    run = mlflow_client.get_run(job_response[\"mlflowRunId\"])  \n",
    "    while(len(run.data.metrics) == 0):\n",
    "        time.sleep(1)\n",
    "        run = mlflow_client.get_run(job_response[\"mlflowRunId\"])\n",
    "        \n",
    "    return run\n",
    "        \n",
    "results = get_mlflow_results(response_le_net_train)\n",
    "pprint.pprint(results.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the baseline training settings, it appears that the LeNet model has been properly trained on MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying and Testing Adversarial Patches\n",
    "\n",
    "Now we will create and apply the adversarial patches over our test set and evaluate the performance of the baseline model on the adversarial patches.\n",
    "\n",
    "We will also apply the patches over the training set for the adversarial training defense evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Generation:\n",
    "The following job will generate the adversarial patches. \n",
    "\n",
    "Feel free to adjust the input parameters to see how they impact the effectiveness of the patch attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch attack (LeNet-5 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T00:58:39.351362',\n",
      " 'dependsOn': 'e6df0b0d-ec6b-4087-a7b1-4ab97064cfc6',\n",
      " 'entryPoint': 'gen_patch',\n",
      " 'entryPointKwargs': '-P model=howard_mnist_adversarial_patches_le_net/1 -P '\n",
      "                     'model_architecture=le_net -P '\n",
      "                     'data_dir=/nfs/data/Mnist/training -P '\n",
      "                     'num_patch_gen_samples=40 -P num_patch=3 -P '\n",
      "                     'patch_target=5',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'e07c4f80-422f-4655-8821-9fb7d9143009',\n",
      " 'lastModified': '2021-02-11T00:58:39.351362',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/dd5cb95d32e244bba00b63821ef958b6/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Patches\n",
    "response_le_net_patches = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"gen_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            f\"-P data_dir={DATASET_DIR}/training\",\n",
    "            \"-P num_patch_gen_samples=40\",\n",
    "            \"-P num_patch=3\",\n",
    "            \"-P patch_target=5\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_train[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch attack (LeNet-5 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_patches)\n",
    "print(\"\")\n",
    "\n",
    "# Wait for Patch attack to finish.\n",
    "response_le_net_patches = wait_until_finished(response_le_net_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Deployment:\n",
    "\n",
    "The following jobs will deploy the patches over the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch deployment (LeNet-5 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T01:01:03.350803',\n",
      " 'dependsOn': 'e07c4f80-422f-4655-8821-9fb7d9143009',\n",
      " 'entryPoint': 'deploy_patch',\n",
      " 'entryPointKwargs': '-P run_id=a29a543b7d2443268af4a6c97254db30 -P '\n",
      "                     'model=howard_mnist_adversarial_patches_le_net/1 -P '\n",
      "                     'model_architecture=le_net -P '\n",
      "                     'data_dir=/nfs/data/Mnist/training',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'f8a6ea17-fe13-4809-940d-9edddb315219',\n",
      " 'lastModified': '2021-02-11T01:01:03.350803',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/2c372deb736d4158aee53c56254a4e56/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deploy Patch attack on training set.\n",
    "response_deploy_le_net_patches_mnist_training = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"deploy_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_le_net_patches['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            f\"-P data_dir={DATASET_DIR}/training\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_patches[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch deployment (LeNet-5 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_deploy_le_net_patches_mnist_training)\n",
    "print(\"\")\n",
    "\n",
    "# Get the run ID of the test set.\n",
    "response_deploy_le_net_patches_mnist_training = get_run_id(response_deploy_le_net_patches_mnist_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch deployment (LeNet-5 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T01:02:02.769583',\n",
      " 'dependsOn': 'e07c4f80-422f-4655-8821-9fb7d9143009',\n",
      " 'entryPoint': 'deploy_patch',\n",
      " 'entryPointKwargs': '-P run_id=a29a543b7d2443268af4a6c97254db30 -P '\n",
      "                     'model=howard_mnist_adversarial_patches_le_net/1 -P '\n",
      "                     'model_architecture=le_net -P '\n",
      "                     'data_dir=/nfs/data/Mnist/testing -P '\n",
      "                     'patch_deployment_method=corrupt',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'e24f6ccb-3ddc-4ba6-8027-17ac45198b4c',\n",
      " 'lastModified': '2021-02-11T01:02:02.769583',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/c7936ae6b0ba4a82b7daa36ba15a4257/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deploy Patch attack on test set.\n",
    "response_deploy_le_net_patches_mnist_testing = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"deploy_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_le_net_patches['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            f\"-P data_dir={DATASET_DIR}/testing\",\n",
    "            \"-P patch_deployment_method=corrupt\"\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_patches[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch deployment (LeNet-5 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_deploy_le_net_patches_mnist_testing)\n",
    "print(\"\")\n",
    "\n",
    "# Get the run ID of the training set.\n",
    "response_deploy_le_net_patches_mnist_testing = get_run_id(response_deploy_le_net_patches_mnist_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Attack Evaluation: Baseline MNIST Model\n",
    "Now we will run an inference step to check the patch-attacked dataset with our MNIST-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch evaluation (LeNet-5 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T01:02:02.769583',\n",
      " 'dependsOn': 'e07c4f80-422f-4655-8821-9fb7d9143009',\n",
      " 'entryPoint': 'deploy_patch',\n",
      " 'entryPointKwargs': '-P run_id=a29a543b7d2443268af4a6c97254db30 -P '\n",
      "                     'model=howard_mnist_adversarial_patches_le_net/1 -P '\n",
      "                     'model_architecture=le_net -P '\n",
      "                     'data_dir=/nfs/data/Mnist/testing -P '\n",
      "                     'patch_deployment_method=corrupt',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'e24f6ccb-3ddc-4ba6-8027-17ac45198b4c',\n",
      " 'lastModified': '2021-02-11T01:02:43.752820',\n",
      " 'mlflowRunId': 'a137b8b5d62b46e8a4b4227af145405a',\n",
      " 'queueId': 2,\n",
      " 'status': 'finished',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/c7936ae6b0ba4a82b7daa36ba15a4257/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check patched dataset results   \n",
    "response_le_net_infer_le_net_patch = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_deploy_le_net_patches_mnist_testing['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            \"-P batch_size=512\",\n",
    "            \"-P dataset_tar_name=adversarial_patch_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_patch_dataset\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_le_net_patches_mnist_testing[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch evaluation (LeNet-5 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_deploy_le_net_patches_mnist_testing)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model results on adversarially patched dataset: \n",
      "{'accuracy': 0.4181690812110901,\n",
      " 'auc': 0.798317015171051,\n",
      " 'fn': 6413.0,\n",
      " 'fp': 375.0,\n",
      " 'loss': 1.650673319513981,\n",
      " 'precision': 0.9049670696258545,\n",
      " 'recall': 0.3576722741127014,\n",
      " 'tn': 89481.0,\n",
      " 'tp': 3571.0}\n"
     ]
    }
   ],
   "source": [
    "# Wait for the job to finish\n",
    "response_le_net_infer_le_net_patch = wait_until_finished(response_le_net_infer_le_net_patch)\n",
    "\n",
    "# Check on the patch evaluation results\n",
    "results = get_mlflow_results(response_le_net_infer_le_net_patch)\n",
    "print(\"Baseline model results on adversarially patched dataset: \")\n",
    "pprint.pprint(results.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that the adversarial patch attack causes a noticeable decrease in the model's accuracy scores.\n",
    "\n",
    "We will now test various defenses against the patch attacked images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defenses: Image Preprocessing and Adversarial Training\n",
    "\n",
    "The next part of the adversarial patch demo focuses on investigating effective defenses against the attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Defenses: Spatial Smoothing, JPEG Compression, Gaussian Augmentation\n",
    "\n",
    "Here we will investigate three preprocessing defenses that can be applied over the images before inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Smoothing Defense:\n",
    "\n",
    "Here, we can adjust the `spatial_smoothing_window_size` parameter to increase or decrease the sliding window of the smoothing defense.\n",
    "\n",
    "Larger values will create more noticeable distortions but can also help mask any adversarial perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial smoothing defense job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T03:25:31.027419',\n",
      " 'dependsOn': 'e24f6ccb-3ddc-4ba6-8027-17ac45198b4c',\n",
      " 'entryPoint': 'spatial_smoothing',\n",
      " 'entryPointKwargs': '-P model=howard_mnist_adversarial_patches_le_net/1 -P '\n",
      "                     'model_architecture=le_net -P '\n",
      "                     'data_dir=/nfs/data/Mnist/training -P batch_size=20 -P '\n",
      "                     'load_dataset_from_mlruns=true -P '\n",
      "                     'spatial_smoothing_window_size=2 -P '\n",
      "                     'dataset_run_id=a137b8b5d62b46e8a4b4227af145405a -P '\n",
      "                     'dataset_tar_name=adversarial_patch_dataset.tar.gz -P '\n",
      "                     'dataset_name=adv_patch_dataset',\n",
      " 'experimentId': 10,\n",
      " 'jobId': '4a03aab8-fc59-464e-a543-b3731808d562',\n",
      " 'lastModified': '2021-02-11T03:25:31.027419',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/cfa8e701e68241db97a214241242fb54/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_le_net_spatial_smoothing_test_set = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"spatial_smoothing\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            f\"-P data_dir={DATASET_DIR}/training\",\n",
    "            \"-P batch_size=20\",\n",
    "            \"-P load_dataset_from_mlruns=true\",\n",
    "            \"-P spatial_smoothing_window_size=2\",\n",
    "            f\"-P dataset_run_id={response_deploy_le_net_patches_mnist_testing['mlflowRunId']}\",\n",
    "            \"-P dataset_tar_name=adversarial_patch_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_patch_dataset\",\n",
    "            \n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_le_net_patches_mnist_testing[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Spatial smoothing defense job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_spatial_smoothing_test_set)\n",
    "print(\"\")\n",
    "\n",
    "response_le_net_spatial_smoothing_test_set = get_run_id(response_le_net_spatial_smoothing_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T03:29:24.413596',\n",
      " 'dependsOn': '4a03aab8-fc59-464e-a543-b3731808d562',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=b3897d27c63f451d88ec9a24e89ff6c3 -P '\n",
      "                     'model=howard_mnist_adversarial_patches_le_net/None -P '\n",
      "                     'model_architecture=le_net -P batch_size=20 -P '\n",
      "                     'dataset_tar_name=spatial_smoothing_dataset.tar.gz -P '\n",
      "                     'dataset_name=adv_testing',\n",
      " 'experimentId': 10,\n",
      " 'jobId': '6d647ac8-f297-43c8-9e84-07852bfacedc',\n",
      " 'lastModified': '2021-02-11T03:29:24.413596',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/8187d9d456914f4aa70c3c38df037302/workflows.tar.gz'}\n",
      "\n",
      "Adversarial Patches with Spatial Smoothing Results: \n",
      "\n",
      "{'accuracy': 0.4125250577926636,\n",
      " 'auc': 0.7922819256782532,\n",
      " 'fn': 6543.0,\n",
      " 'fp': 592.0,\n",
      " 'loss': 1.6688883025676777,\n",
      " 'precision': 0.8530652523040771,\n",
      " 'recall': 0.34438878297805786,\n",
      " 'tn': 89228.0,\n",
      " 'tp': 3437.0}\n"
     ]
    }
   ],
   "source": [
    "# Wait for defense to complete, then check baseline model on defended test set with adversarial patches.\n",
    "\n",
    "response_evaluate_spatial_smoothing_images = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_le_net_spatial_smoothing_test_set['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/None\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            \"-P batch_size=20\",\n",
    "            \"-P dataset_tar_name=spatial_smoothing_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_testing\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_spatial_smoothing_test_set[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Inference job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_evaluate_spatial_smoothing_images)\n",
    "print(\"\")\n",
    "\n",
    "response_evaluate_spatial_smoothing_images = wait_until_finished(response_evaluate_spatial_smoothing_images)\n",
    "results = get_mlflow_results(response_evaluate_spatial_smoothing_images)\n",
    "print(\"Adversarial Patches with Spatial Smoothing Results: \\n\")\n",
    "pprint.pprint(results.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that spatial smoothing does not protect well against adversarial patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPEG Compression Defense:\n",
    "\n",
    "We can adjust the image compression quality by modifying the `jpeg_compression_quality` field. \n",
    "Enter any value between 1 (worst) to 95 (best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPEG compression defense job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T04:04:29.125973',\n",
      " 'dependsOn': 'e24f6ccb-3ddc-4ba6-8027-17ac45198b4c',\n",
      " 'entryPoint': 'jpeg_compression',\n",
      " 'entryPointKwargs': '-P model=howard_mnist_adversarial_patches_le_net/1 -P '\n",
      "                     'model_architecture=le_net -P '\n",
      "                     'data_dir=/nfs/data/ImageNet-Kaggle-2017/images/ILSVRC/Data/CLS-LOC/val-sorted-1000 '\n",
      "                     '-P batch_size=20 -P load_dataset_from_mlruns=true -P '\n",
      "                     'dataset_run_id=a137b8b5d62b46e8a4b4227af145405a -P '\n",
      "                     'jpeg_compression_quality=30 -P '\n",
      "                     'dataset_tar_name=adversarial_patch_dataset.tar.gz -P '\n",
      "                     'dataset_name=adv_patch_dataset',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'd9b6063d-f27d-473a-96ec-26de8fb48d8a',\n",
      " 'lastModified': '2021-02-11T04:04:29.125973',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/6e83977d6e5a48978a8a44cebaaf1145/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_le_net_jpeg_compression_test_set = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"jpeg_compression\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            f\"-P data_dir=/{DATASET_DIR}/training\",\n",
    "            \"-P batch_size=20\",\n",
    "            \"-P load_dataset_from_mlruns=true\",\n",
    "            f\"-P dataset_run_id={response_deploy_le_net_patches_mnist_testing['mlflowRunId']}\",\n",
    "            \"-P jpeg_compression_quality=30\",\n",
    "            \"-P dataset_tar_name=adversarial_patch_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_patch_dataset\",\n",
    "            \n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_le_net_patches_mnist_testing[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"JPEG compression defense job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_jpeg_compression_test_set)\n",
    "print(\"\")\n",
    "\n",
    "response_le_net_jpeg_compression_test_set = get_run_id(response_le_net_jpeg_compression_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T04:04:33.250324',\n",
      " 'dependsOn': 'd9b6063d-f27d-473a-96ec-26de8fb48d8a',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=2233c7370cbc4a7dac3d390b0161ddcd -P '\n",
      "                     'model=howard_mnist_adversarial_patches_le_net/None -P '\n",
      "                     'model_architecture=le_net -P batch_size=20 -P '\n",
      "                     'dataset_tar_name=jpeg_compression_dataset.tar.gz -P '\n",
      "                     'dataset_name=adv_testing',\n",
      " 'experimentId': 10,\n",
      " 'jobId': '1c6c6456-0e4d-4261-9b69-c1c37248d15e',\n",
      " 'lastModified': '2021-02-11T04:04:33.250324',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/e385f2da8f8e4192a91a80eb6d5dd242/workflows.tar.gz'}\n",
      "\n",
      "Adversarial Patches with JPEG Compression Results: \n",
      "\n",
      "{'accuracy': 0.4106212556362152,\n",
      " 'auc': 0.7881637215614319,\n",
      " 'fn': 6438.0,\n",
      " 'fp': 1159.0,\n",
      " 'loss': 1.8545305996369092,\n",
      " 'precision': 0.753456711769104,\n",
      " 'recall': 0.3549098074436188,\n",
      " 'tn': 88661.0,\n",
      " 'tp': 3542.0}\n"
     ]
    }
   ],
   "source": [
    "# Wait for defense to complete, then check baseline model on defended test set with adversarial patches.\n",
    "\n",
    "response_evaluate_jpeg_compression_images = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_le_net_jpeg_compression_test_set['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/None\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            \"-P batch_size=20\",\n",
    "            \"-P dataset_tar_name=jpeg_compression_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_testing\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_jpeg_compression_test_set[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Inference job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_evaluate_jpeg_compression_images)\n",
    "print(\"\")\n",
    "\n",
    "response_evaluate_jpeg_compression_images = wait_until_finished(response_evaluate_jpeg_compression_images)\n",
    "results = get_mlflow_results(response_evaluate_jpeg_compression_images)\n",
    "print(\"Adversarial Patches with JPEG Compression Results: \\n\")\n",
    "pprint.pprint(results.data.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Augmentation Defense:\n",
    "\n",
    "We can adjust the amount of noise created by the defense by adjusting the `gaussian_augmentation_sigma` parameter. \n",
    "Please enter any positive value for sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Augmentation defense job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T04:09:31.863074',\n",
      " 'dependsOn': 'e24f6ccb-3ddc-4ba6-8027-17ac45198b4c',\n",
      " 'entryPoint': 'gaussian_augmentation',\n",
      " 'entryPointKwargs': '-P model=howard_mnist_adversarial_patches_le_net/1 -P '\n",
      "                     'model_architecture=le_net -P '\n",
      "                     'data_dir=/nfs/data/ImageNet-Kaggle-2017/images/ILSVRC/Data/CLS-LOC/val-sorted-1000 '\n",
      "                     '-P batch_size=20 -P load_dataset_from_mlruns=true -P '\n",
      "                     'dataset_run_id=a137b8b5d62b46e8a4b4227af145405a -P '\n",
      "                     'dataset_tar_name=adversarial_patch_dataset.tar.gz -P '\n",
      "                     'dataset_name=adv_patch_dataset -P '\n",
      "                     'gaussian_augmentation_sigma=0.3',\n",
      " 'experimentId': 10,\n",
      " 'jobId': '099c20a4-6549-4c7c-9153-6b90d3dc8c19',\n",
      " 'lastModified': '2021-02-11T04:09:31.863074',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/bfb1fe1e31084d84b5d327bace64aa9d/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_le_net_gaussian_augmentation_test_set = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"gaussian_augmentation\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            f\"-P data_dir=/{DATASET_DIR}/training\",\n",
    "            \"-P batch_size=20\",\n",
    "            \"-P load_dataset_from_mlruns=true\",\n",
    "            f\"-P dataset_run_id={response_deploy_le_net_patches_mnist_testing['mlflowRunId']}\",\n",
    "            \"-P dataset_tar_name=adversarial_patch_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_patch_dataset\",\n",
    "            \"-P gaussian_augmentation_sigma=0.3\"\n",
    "            \n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_le_net_patches_mnist_testing[\"jobId\"],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Gaussian Augmentation defense job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_gaussian_augmentation_test_set)\n",
    "print(\"\")\n",
    "\n",
    "response_le_net_gaussian_augmentation_test_set = get_run_id(response_le_net_gaussian_augmentation_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T04:10:53.605176',\n",
      " 'dependsOn': '099c20a4-6549-4c7c-9153-6b90d3dc8c19',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=a3d8b1d2fbd9430b916b3072f5295667 -P '\n",
      "                     'model=howard_mnist_adversarial_patches_le_net/None -P '\n",
      "                     'model_architecture=le_net -P batch_size=20 -P '\n",
      "                     'dataset_tar_name=gaussian_augmentation_dataset.tar.gz -P '\n",
      "                     'dataset_name=adv_testing',\n",
      " 'experimentId': 10,\n",
      " 'jobId': '1896a83c-648f-4f1c-9070-5e88696542b2',\n",
      " 'lastModified': '2021-02-11T04:10:53.605176',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/daf5e513206a44988ccc04adb32631de/workflows.tar.gz'}\n",
      "\n",
      "Adversarial Patches with Gaussian Augmentation Results: \n",
      "\n",
      "{'accuracy': 0.35891783237457275,\n",
      " 'auc': 0.7413415312767029,\n",
      " 'fn': 7019.0,\n",
      " 'fp': 2055.0,\n",
      " 'loss': 2.278626961585803,\n",
      " 'precision': 0.5903109908103943,\n",
      " 'recall': 0.2966933846473694,\n",
      " 'tn': 87765.0,\n",
      " 'tp': 2961.0}\n"
     ]
    }
   ],
   "source": [
    "# Wait for defense to complete, then check baseline model on defended test set with adversarial patches.\n",
    "\n",
    "response_evaluate_gaussian_augmentation_images = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_le_net_gaussian_augmentation_test_set['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_le_net/None\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            \"-P batch_size=20\",\n",
    "            \"-P dataset_tar_name=gaussian_augmentation_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_testing\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_le_net_gaussian_augmentation_test_set[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Inference job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_evaluate_gaussian_augmentation_images)\n",
    "print(\"\")\n",
    "\n",
    "response_evaluate_gaussian_augmentation_images = wait_until_finished(response_evaluate_gaussian_augmentation_images)\n",
    "results = get_mlflow_results(response_evaluate_gaussian_augmentation_images)\n",
    "print(\"Adversarial Patches with Gaussian Augmentation Results: \\n\")\n",
    "pprint.pprint(results.data.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that of the three preprocessing defenses, gaussian augmentation might actually improve patch attach effectiveness.\n",
    "\n",
    "Since preprocessing defenses appear to be ineffective at stopping adversarial patches, let's try another approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training Defense:\n",
    "\n",
    "Finally, we will train a new copy of the LeNet model on training set that contains adversarial patches.\n",
    "In doing so, the model learns to ignore the adversarial patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch adversarial training (LeNet-5 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T05:17:29.520627',\n",
      " 'dependsOn': 'f8a6ea17-fe13-4809-940d-9edddb315219',\n",
      " 'entryPoint': 'train',\n",
      " 'entryPointKwargs': '-P '\n",
      "                     'testing_dataset_run_id=a137b8b5d62b46e8a4b4227af145405a '\n",
      "                     '-P '\n",
      "                     'training_dataset_run_id=b1886b21e58845759fb43537709f99b4 '\n",
      "                     '-P batch_size=256 -P register_model=True -P '\n",
      "                     'model_architecture=le_net -P model_tag=adversarial_patch '\n",
      "                     '-P epochs=30 -P data_dir_train=/nfs/data/Mnist/training '\n",
      "                     '-P data_dir_test=/nfs/data/Mnist/testing -P '\n",
      "                     'load_dataset_from_mlruns=True',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'deba691a-edf1-49ab-9c3a-46eea3254923',\n",
      " 'lastModified': '2021-02-11T05:17:29.520627',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/1f8071a6e81b4a07ad9e4bb1150c97d1/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally, train and retest patched dataset.\n",
    "\n",
    "response_deploy_le_net_patches_mnist_adv_training = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P testing_dataset_run_id={response_deploy_le_net_patches_mnist_testing['mlflowRunId']}\",\n",
    "            f\"-P training_dataset_run_id={response_deploy_le_net_patches_mnist_training['mlflowRunId']}\",\n",
    "            \"-P batch_size=256\",\n",
    "            \"-P register_model=True\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            \"-P model_tag=adversarial_patch\",\n",
    "            \"-P epochs=30\",\n",
    "            f\"-P data_dir_train={DATASET_DIR}/training\",\n",
    "            f\"-P data_dir_test={DATASET_DIR}/testing\",\n",
    "            \"-P load_dataset_from_mlruns=True\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_le_net_patches_mnist_training[\"jobId\"],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Patch adversarial training (LeNet-5 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_deploy_le_net_patches_mnist_adv_training)\n",
    "print(\"\")\n",
    "\n",
    "response_deploy_le_net_patches_mnist_adv_training = get_run_id(response_deploy_le_net_patches_mnist_adv_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch evaluation job submitted\n",
      "\n",
      "{'createdOn': '2021-02-11T05:20:36.058379',\n",
      " 'dependsOn': 'deba691a-edf1-49ab-9c3a-46eea3254923',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=a137b8b5d62b46e8a4b4227af145405a -P '\n",
      "                     'model=howard_mnist_adversarial_patches_adversarial_patch_le_net/None '\n",
      "                     '-P model_architecture=le_net -P batch_size=20 -P '\n",
      "                     'dataset_tar_name=adversarial_patch_dataset.tar.gz -P '\n",
      "                     'dataset_name=adv_patch_dataset',\n",
      " 'experimentId': 10,\n",
      " 'jobId': 'bf197423-b636-4788-9081-e55ff7c97e5d',\n",
      " 'lastModified': '2021-02-11T05:20:36.058379',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/152cae393b1e41f88015dfdce644afa8/workflows.tar.gz'}\n",
      "\n",
      "Adversarial Training Results:\n"
     ]
    }
   ],
   "source": [
    "response_evaluate_adv_training = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_deploy_le_net_patches_mnist_testing['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_adversarial_patch_le_net/None\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "            \"-P batch_size=20\",\n",
    "            \"-P dataset_tar_name=adversarial_patch_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_patch_dataset\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_le_net_patches_mnist_adv_training[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch evaluation job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_evaluate_adv_training)\n",
    "print(\"\")\n",
    "\n",
    "response_evaluate_adv_training = wait_until_finished(response_evaluate_adv_training)\n",
    "results = get_mlflow_results(response_evaluate_adv_training)\n",
    "print(\"Adversarial Training Results:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that for adversarial patches, adversarial training appears to be a good option.\n",
    "### Some other interesting approaches that users may want to investigate by modfifying this demo are:\n",
    "\n",
    " - Generating a separate set of patches for training and test data. \n",
    "    - To do so, add a secondary patch generation job and link the run_id of this new job to one of the existing test/training patch deployment jobs. \n",
    "    \n",
    " - Experimenting with alternate adversarially trained models. \n",
    "     - If both the MNIST patch demo and MNIST FGM demos are run together, then users can swap the `model={EXPERIMENT_NAME}_adversarial_patch_le_net/None` with the model name of the FGM experiment.\n",
    "     - Doing so can let users quickly check which adversarial training offers robust protection against multiple types of attacks. \n",
    "     \n",
    "### Please consult the README documentation for more information regarding available entrypoints and attack/defense parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='querying_cell'></a>\n",
    "## Querying the MLFlow Tracking Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the lab API can only be used to register experiments and start jobs, so if users wish to extract their results programmatically, they can use the `MlflowClient()` class from the `mlflow` Python package to connect and query their results.\n",
    "Since we captured the run ids generated by MLFlow, we can easily retrieve the data logged about one of our jobs and inspect the results.\n",
    "To start the client, we simply need to run,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client uses the environment variable `MLFLOW_TRACKING_URI` to figure out how to connect to the MLFlow Tracking Service, which we configured near the top of this notebook.\n",
    "To query the results of one of our runs, we just need to pass the run id to the client's `get_run()` method.\n",
    "As an example, let's query the run results for the patch attack applied to the LeNet-5 architecture,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_le_net = mlflow_client.get_run(response_le_net_patches[\"mlflowRunId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the request completed successfully, we should now be able to query data collected during the run.\n",
    "For example, to review the collected metrics, we just use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_le_net.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the run's parameters, we use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_le_net.data.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the run's tags, we use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_le_net.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things you can query using the MLFlow client.\n",
    "[The MLFlow documentation gives a full overview of the methods that are available](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
