{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST Classifier demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an end-to-end demostration of the Securing AI Lab Architecture that can be run on any modern laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:30080\"\n",
    "\n",
    "# Base API address\n",
    "RESTAPI_API_BASE = f\"{RESTAPI_ADDRESS}/api\"\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\"\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "import requests\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Import utils.py file\n",
    "import utils\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the Makefile works in your environment by executing the `bash` code block below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Running this will just list the available rules defined in the demo's Makefile.\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to download the MNIST dataset for the demo, saving the training and testing images to the `data/` directory as PNG files that are organized into the following folder structure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    data\n",
    "    ├── testing\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   ├── 4\n",
    "    │   ├── 5\n",
    "    │   ├── 6\n",
    "    │   ├── 7\n",
    "    │   ├── 8\n",
    "    │   └── 9\n",
    "    └── training\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── 4\n",
    "        ├── 5\n",
    "        ├── 6\n",
    "        ├── 7\n",
    "        ├── 8\n",
    "        └── 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subfolders under `data/training/` and `data/testing/` are the classification labels for the images in the dataset.\n",
    "This folder structure is a standardized way to encode the label information and many libraries can make use of it, including the Tensorflow library that we are using for this particular demo.\n",
    "For your convenience, a Python script and Makefile rule are provided that will automatically download the images as PNG images and organize them into this folder layout, just execute the `bash` code block below.\n",
    "Please note that it will take a couple of minutes for the PNG exporting and folder sorting to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# If you have already run this command, you should just see the message:\n",
    "#\n",
    "#     make: Nothing to be done for `data'.\n",
    "\n",
    "make data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sampling of the training images we just downloaded to confirm that there aren't any problems,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_training_samples = sorted(\n",
    "    rng.choice(list(Path(\"data/training\").glob(\"*/*.png\")), size=20).tolist()\n",
    ")\n",
    "utils.notebook_gallery(mnist_training_samples, row_height=\"140px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the images downloaded correctly, you should see a set of images that look like hand-written numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy microservices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the demo, we first need to set up and deploy all the services needed to run the demo.\n",
    "This requires that all the Docker images needed for the Securing AI Lab environment are built or pulled into your environment.\n",
    "If you have not done so already, install Docker and then run `make build-all` to build all the images.\n",
    "A high-level schematic showing how all of the images connect together to form the Securing AI Lab Architecture is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](securing_ai_lab_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, the startup sequence for deploying the services is implemented in the accompanying `Makefile` for this demo.\n",
    "To deploy the services, simply run `make services`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Deploy the lab microservices declared in docker-compose.yml\n",
    "make services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jobs that we will be running are implemented in the Python source files under `src/`, which will be executed using the entrypoints defined in the `MLproject` file.\n",
    "To get this information into the architecture, we need to package those files up into an archive and upload it to the lab API.\n",
    "For convenience, the `Makefile` provides a rule for creating the archive file, just run `make workflows`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `utils.py` file that is able to connect with the lab's RESTful API using the HTTP protocol.\n",
    "We connect using the client below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restapi_client = utils.SecuringAIClient(address=RESTAPI_API_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment named `\"mnist\"` exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_experiment = restapi_client.get_experiment_by_name(name=\"mnist\")\n",
    "\n",
    "if response_experiment is None or \"Not Found\" in response_experiment.get(\"message\", []):\n",
    "    response_experiment = restapi_client.register_experiment(name=\"mnist\")\n",
    "\n",
    "response_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to train our model.\n",
    "Depending on the specs of your computer, training either the shallow net model or the LeNet-5 model on a CPU can take 10-20 minutes or longer to complete.\n",
    "If you are fortunate enough to have access to a dedicated GPU, then the training time will be much shorter.\n",
    "So that we do not start this code by accident, we are embedding the code in a text block instead of keeping it in an executable code block.\n",
    "If you need to train one of the models, create a new code block and copy and paste the code into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Submit training job for the shallow network architecture\n",
    "response_shallow_train = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=\"mnist\",\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "        \"-P register_model=True\",\n",
    "        \"-P model_architecture=shallow_net\",\n",
    "        \"-P epochs=30\",\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Training job for shallow neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_shallow_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Submit training job for the LeNet-5 network architecture\n",
    "response_le_net_train = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=\"mnist\",\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "        \"-P register_model=True\",\n",
    "        \"-P model_architecture=le_net\",\n",
    "        \"-P epochs=30\",\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Training job for LeNet-5 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two trained models (the shallow network and the LeNet-5 network), next we will apply the fast-gradient method (FGM) evasion attack on the shallow network to generate adversarial images.\n",
    "Then, after we have the adversarial images, we will use them to evaluate some standard machine learning metrics against both models.\n",
    "This will give us a sense of the transferability of the attacks between models.\n",
    "\n",
    "This specific workflow is an example of jobs that contain dependencies, as the metric evaluation jobs cannot start until the adversarial image generation jobs have completed.\n",
    "The lab architecture allows users to declare one-to-many job dependencies like this, which we will use to queue up jobs to start immediately after the previous jobs have concluded.\n",
    "The code below illustrates this by doing the following:\n",
    "\n",
    "1. A job is submitted that generates adversarial images based on the shallow net architecture (entry point **fgm**).\n",
    "1. We wait until the job starts and a MLFlow identifier is assigned, which we check by polling the API until we see the id appear.\n",
    "1. Once we have an id returned to us from the API, we queue up the metrics evaluation jobs and declare the job dependency using the `depends_on` option.\n",
    "1. The message \"Dependent jobs submitted\" will display once everything is queued up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_run_id_is_not_known(response_fgm):\n",
    "    return response_fgm[\"mlflowRunId\"] is None and response_fgm[\"status\"] not in [\n",
    "        \"failed\",\n",
    "        \"finished\",\n",
    "    ]\n",
    "\n",
    "response_fgm_shallow_net = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=\"mnist\",\n",
    "    entry_point=\"fgm\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\"-P model=mnist_shallow_net/1\", \"-P model_architecture=shallow_net\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"FGM attack (shallow net architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_fgm_shallow_net)\n",
    "print(\"\")\n",
    "\n",
    "while mlflow_run_id_is_not_known(response_fgm_shallow_net):\n",
    "    time.sleep(1)\n",
    "    response_fgm_shallow_net = restapi_client.get_job_by_id(\n",
    "        response_fgm_shallow_net[\"jobId\"]\n",
    "    )\n",
    "\n",
    "response_shallow_net_infer_shallow_net = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=\"mnist\",\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_fgm_shallow_net['mlflowRunId']}\",\n",
    "            \"-P model=mnist_shallow_net/1\",\n",
    "            \"-P model_architecture=shallow_net\",\n",
    "        ]\n",
    "    ),\n",
    "    depends_on=response_fgm_shallow_net[\"jobId\"],\n",
    ")\n",
    "\n",
    "response_le_net_infer_shallow_net = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=\"mnist\",\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_fgm_shallow_net['mlflowRunId']}\",\n",
    "            \"-P model=mnist_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "        ]\n",
    "    ),\n",
    "    depends_on=response_fgm_shallow_net[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Dependent jobs submitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can poll the status of the dependent jobs using the code below.\n",
    "We should see the status of the jobs shift from \"queued\" to \"started\" and eventually become \"finished\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_shallow_net_infer_shallow_net = restapi_client.get_job_by_id(\n",
    "    response_shallow_net_infer_shallow_net[\"jobId\"]\n",
    ")\n",
    "response_le_net_infer_shallow_net = restapi_client.get_job_by_id(\n",
    "    response_le_net_infer_shallow_net[\"jobId\"]\n",
    ")\n",
    "\n",
    "pprint.pprint(response_shallow_net_infer_shallow_net)\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_infer_shallow_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similiarly run an FGM-based evasion attack using the LeNet-5 architecture as our starting point.\n",
    "The following code is very similar to the code we just saw, all we've done is swap out the entry point keyword argument that requests the shallow net architecture with a version that requests the LeNet-5 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_fgm_le_net = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=\"mnist\",\n",
    "    entry_point=\"fgm\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\"-P model=mnist_le_net/1\", \"-P model_architecture=le_net\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"FGM attack (LeNet-5 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_fgm_le_net)\n",
    "print(\"\")\n",
    "\n",
    "while mlflow_run_id_is_not_known(response_fgm_le_net):\n",
    "    time.sleep(1)\n",
    "    response_fgm_le_net = restapi_client.get_job_by_id(response_fgm_le_net[\"jobId\"])\n",
    "\n",
    "response_shallow_net_infer_le_net_fgm = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=\"mnist\",\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_fgm_le_net['mlflowRunId']}\",\n",
    "            \"-P model=mnist_shallow_net/1\",\n",
    "            \"-P model_architecture=shallow_net\",\n",
    "        ]\n",
    "    ),\n",
    "    depends_on=response_fgm_le_net[\"jobId\"],\n",
    ")\n",
    "\n",
    "response_le_net_infer_le_net_fgm = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=\"mnist\",\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_fgm_le_net['mlflowRunId']}\",\n",
    "            \"-P model=mnist_le_net/1\",\n",
    "            \"-P model_architecture=le_net\",\n",
    "        ]\n",
    "    ),\n",
    "    depends_on=response_fgm_le_net[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Dependent jobs submitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we can monitor the status of the dependent jobs by querying the API using the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_shallow_net_infer_le_net_fgm = restapi_client.get_job_by_id(\n",
    "    response_shallow_net_infer_le_net_fgm[\"jobId\"]\n",
    ")\n",
    "response_le_net_infer_le_net_fgm = restapi_client.get_job_by_id(\n",
    "    response_le_net_infer_le_net_fgm[\"jobId\"]\n",
    ")\n",
    "\n",
    "pprint.pprint(response_shallow_net_infer_le_net_fgm)\n",
    "print(\"\")\n",
    "pprint.pprint(response_le_net_infer_le_net_fgm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've just run your first experiment in the Securing AI Lab Architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the MLFlow Tracking Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the lab API can only be used to register experiments and start jobs, so if users wish to extract their results programmatically, they can use the `MlflowClient()` class from the `mlflow` Python package to connect and query their results.\n",
    "Since we captured the run ids generated by MLFlow, we can easily retrieve the data logged about one of our jobs and inspect the results.\n",
    "To start the client, we simply need to run,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client uses the environment variable `MLFLOW_TRACKING_URI` to figure out how to connect to the MLFlow Tracking Service, which we configured near the top of this notebook.\n",
    "To query the results of one of our runs, we just need to pass the run id to the client's `get_run()` method.\n",
    "As an example, let's query the run results for the FGM attack applied to the LeNet-5 architecture,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgm_run_le_net = mlflow_client.get_run(response_fgm_le_net[\"mlflowRunId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the request completed successfully, we should now be able to query data collected during the run.\n",
    "For example, to review the collected metrics, we just use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(fgm_run_le_net.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the run's parameters, we use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(fgm_run_le_net.data.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the run's tags, we use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(fgm_run_le_net.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things you can query using the MLFlow client.\n",
    "[The MLFlow documentation gives a full overview of the methods that are available](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean up, you simply need to shut down the services, which can be done using the following Makefile rule,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Stop and remove all microservices\n",
    "make teardown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
