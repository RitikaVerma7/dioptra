{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Basic demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a lightweight demonstration of the current Securing AI Lab demo setup with MLflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "\n",
    "# Please enter custom username here.\n",
    "USERNAME = \"howard\"\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:30080\"\n",
    "\n",
    "# Base API address\n",
    "RESTAPI_API_BASE = f\"{RESTAPI_ADDRESS}/api\"\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\"\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Experiment name (note the username_ prefix convention)\n",
    "EXPERIMENT_NAME = f\"{USERNAME}_basic\"\n",
    "\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "import requests\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Import utils.py file\n",
    "import utils\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the Makefile works in your environment by executing the `bash` code block below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAvailable rules:\u001b[m\n",
      "\n",
      "\u001b[36mclean              \u001b[m Remove temporary files \n",
      "\u001b[36mdata               \u001b[m Download and prepare MNIST dataset \n",
      "\u001b[36minitdb             \u001b[m Initialize the RESTful API database \n",
      "\u001b[36mservices           \u001b[m Launch the Minio S3 and MLFlow Tracking services \n",
      "\u001b[36mteardown           \u001b[m Destroy service containers \n",
      "\u001b[36mworkflows          \u001b[m Create workflows tarball \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhuang/.conda/envs/tensorflow-mnist-classifier/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Running this will just list the available rules defined in the demo's Makefile.\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jobs that we will be running are implemented in the Python source files under `src/`, which will be executed using the entrypoints defined in the `MLproject` file.\n",
    "To get this information into the architecture, we need to package those files up into an archive and upload it to the lab API.\n",
    "For convenience, the `Makefile` provides a rule for creating the archive file, just run `make workflows`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar czf workflows.tar.gz src/load_model_dataset.py src/log.py src/hello_world.py MLproject\n",
      "chmod 644 workflows.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhuang/.conda/envs/tensorflow-mnist-classifier/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `utils.py` file that is able to connect with the lab's RESTful API using the HTTP protocol.\n",
    "We connect using the client below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhuang/.conda/envs/tensorflow-mnist-classifier/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "restapi_client = utils.SecuringAIClient(address=RESTAPI_API_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment named `\"basic\"` exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Demo: Defining Job Parameters:\n",
    "\n",
    "Here we will submit a basic job through MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experimentId': 17,\n",
       " 'name': 'howard_basic',\n",
       " 'lastModified': '2020-12-11T10:58:54.178417',\n",
       " 'createdOn': '2020-12-11T10:58:54.178417'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_experiment = restapi_client.get_experiment_by_name(name=EXPERIMENT_NAME)\n",
    "\n",
    "if response_experiment is None or \"Not Found\" in response_experiment.get(\"message\", []):\n",
    "    response_experiment = restapi_client.register_experiment(name=EXPERIMENT_NAME)\n",
    "\n",
    "response_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic job submitted.\n",
      "\n",
      "{'createdOn': '2021-03-27T09:17:20.586719',\n",
      " 'dependsOn': None,\n",
      " 'entryPoint': 'hello_world',\n",
      " 'entryPointKwargs': None,\n",
      " 'experimentId': 17,\n",
      " 'jobId': 'f1a56642-6ff2-41c1-8ed2-b24094fb889f',\n",
      " 'lastModified': '2021-03-27T09:17:20.586719',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 1,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/ab94c87e786d49a1925b9269be5a39c3/workflows.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "# Submit baseline job:          \n",
    "basic_job = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"hello_world\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Basic job submitted.\")\n",
    "print(\"\")\n",
    "pprint.pprint(basic_job)\n",
    "\n",
    "\n",
    "def mlflow_run_id_is_not_known(response):\n",
    "    return response[\"mlflowRunId\"] is None and response[\"status\"] not in [\n",
    "        \"failed\",\n",
    "        \"finished\",\n",
    "    ]\n",
    "\n",
    "# Retrieve mlflow run_id\n",
    "while mlflow_run_id_is_not_known(basic_job):\n",
    "    time.sleep(1)\n",
    "    basic_job = restapi_client.get_job_by_id(basic_job[\"jobId\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can query the job to view its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_log_string': \"'Hello World'\"}\n",
      "{'mlflow.project.entryPoint': 'hello_world',\n",
      " 'mlflow.source.name': '/work/tmprljlc28b',\n",
      " 'mlflow.source.type': 'PROJECT',\n",
      " 'mlflow.user': 'securingai',\n",
      " 'securingai.dependsOn': 'None',\n",
      " 'securingai.jobId': 'f1a56642-6ff2-41c1-8ed2-b24094fb889f',\n",
      " 'securingai.queue': 'tensorflow_cpu'}\n"
     ]
    }
   ],
   "source": [
    "# Next we can see the baseline output from the job:\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "basic_job_query  = mlflow_client.get_run(basic_job[\"mlflowRunId\"])\n",
    "\n",
    "pprint.pprint(basic_job_query.data.params)\n",
    "pprint.pprint(basic_job_query.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To customize job parameters: \n",
    "\n",
    "Add `-P job_property=<job_value>` to the entry_point_kwargs field in the job submission script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic job submitted.\n",
      "\n",
      "{'createdOn': '2021-03-27T09:17:39.315095',\n",
      " 'dependsOn': None,\n",
      " 'entryPoint': 'hello_world',\n",
      " 'entryPointKwargs': '-P output_log_string=\"Hello_again!\"',\n",
      " 'experimentId': 17,\n",
      " 'jobId': 'c1b7bdec-3955-4552-89be-7d4d1ab5c16a',\n",
      " 'lastModified': '2021-03-27T09:17:39.315095',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 1,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/25f5726bb3294904abe6eab24d9b852d/workflows.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "# Submit baseline job:          \n",
    "basic_job = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"hello_world\",\n",
    "    entry_point_kwargs=' '.join([\n",
    "        '-P output_log_string=\"Hello_again!\"'\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Basic job submitted.\")\n",
    "print(\"\")\n",
    "pprint.pprint(basic_job)\n",
    "\n",
    "\n",
    "# Retrieve mlflow run_id\n",
    "while mlflow_run_id_is_not_known(basic_job):\n",
    "    time.sleep(1)\n",
    "    basic_job = restapi_client.get_job_by_id(basic_job[\"jobId\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_log_string': \"'Hello_again!'\"}\n"
     ]
    }
   ],
   "source": [
    "# Next we can see the baseline output from the job. The output has changed due to the new user parameter.\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "basic_job_query  = mlflow_client.get_run(basic_job[\"mlflowRunId\"])\n",
    "\n",
    "pprint.pprint(basic_job_query.data.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
