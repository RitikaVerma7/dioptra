{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Fruits360 Patch demo for Securing AI Lab deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an end-to-end demonstration for the Securing AI Lab Architecture when it is deployed on the DGX workstation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This demo is specifically for the NCCoE DGX Workstation with hostname `dgx-station-2`.\n",
    "\n",
    "Port forwarding is required in order to run this demo.\n",
    "The recommended port mapping is as follows:\n",
    "\n",
    "- Map `localhost:30080` on laptop to `localhost:30080` on `dgx-station-2`\n",
    "- Map `localhost:35000` on laptop to `localhost:35000` on `dgx-station-2`\n",
    "\n",
    "A sample SSH config file that enables the above port forwarding is provided below,\n",
    "\n",
    "> ⚠️ **Edits required**: replace `username` with your assigned username _on the NCCoE virtual machines_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```conf\n",
    "# vm hostname: jumphost001\n",
    "Host nccoe-jumphost001\n",
    "    Hostname 10.33.53.98\n",
    "    User username\n",
    "    Port 54131\n",
    "    IdentityFile %d/.ssh/nccoe-vm\n",
    "\n",
    "# vm hostname: dgx-station-2\n",
    "Host nccoe-k8s-gpu002\n",
    "    Hostname 192.168.1.28\n",
    "    User username\n",
    "    Port 22\n",
    "    IdentityFile %d/.ssh/nccoe-vm\n",
    "    ProxyJump nccoe-jumphost001\n",
    "    LocalForward 30080 localhost:30080\n",
    "    LocalForward 35000 localhost:35000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, connect to the NCCoE VPN and SSH into the DGX Workstation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "ssh nccoe-k8s-gpu002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Please enter custom username here.\n",
    "USERNAME = \"howard\"\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:30080\"\n",
    "\n",
    "# Base API address\n",
    "RESTAPI_API_BASE = f\"{RESTAPI_ADDRESS}/api\"\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\"\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Experiment name (note the username_ prefix convention)\n",
    "EXPERIMENT_NAME = f\"{USERNAME}_fruits360_adversarial_patches\"\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "import requests\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Import utils.py file\n",
    "import utils\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the Makefile works in your environment by executing the `bash` code block below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAvailable rules:\u001b[m\n",
      "\n",
      "\u001b[36mclean              \u001b[m Remove temporary files \n",
      "\u001b[36mdata               \u001b[m Download and prepare MNIST dataset \n",
      "\u001b[36minitdb             \u001b[m Initialize the RESTful API database \n",
      "\u001b[36mservices           \u001b[m Launch the Minio S3 and MLFlow Tracking services \n",
      "\u001b[36mteardown           \u001b[m Destroy service containers \n",
      "\u001b[36mworkflows          \u001b[m Create workflows tarball \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Running this will just list the available rules defined in the demo's Makefile.\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jobs that we will be running are implemented in the Python source files under `src/`, which will be executed using the entrypoints defined in the `MLproject` file.\n",
    "To get this information into the architecture, we need to package those files up into an archive and upload it to the lab API.\n",
    "For convenience, the `Makefile` provides a rule for creating the archive file, just run `make workflows`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'workflows'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `utils.py` file that is able to connect with the lab's RESTful API using the HTTP protocol.\n",
    "We connect using the client below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "restapi_client = utils.SecuringAIClient(address=RESTAPI_API_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experimentId': 11,\n",
       " 'name': 'howard_fruits360_adversarial_patches',\n",
       " 'lastModified': '2020-11-05T09:37:19.652250',\n",
       " 'createdOn': '2020-11-05T09:37:19.652250'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_experiment = restapi_client.get_experiment_by_name(name=EXPERIMENT_NAME)\n",
    "\n",
    "if response_experiment is None or \"Not Found\" in response_experiment.get(\"message\", []):\n",
    "    response_experiment = restapi_client.register_experiment(name=EXPERIMENT_NAME)\n",
    "\n",
    "response_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to train our model.\n",
    "We will be using the V100 GPUs that are available on the DGX Workstation, which we can use by submitting our job to the `\"tensorflow_gpu\"` queue.\n",
    "We will train a VGG16 model on the Fruits360 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job for VGG16 neural network submitted\n",
      "\n",
      "{'createdOn': '2021-01-05T19:03:52.277837',\n",
      " 'dependsOn': None,\n",
      " 'entryPoint': 'train',\n",
      " 'entryPointKwargs': '-P batch_size=20 -P register_model=True -P '\n",
      "                     'model_architecture=vgg16 -P epochs=30 -P '\n",
      "                     'data_dir_train=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Training '\n",
      "                     '-P '\n",
      "                     'data_dir_test=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Test',\n",
      " 'experimentId': 11,\n",
      " 'jobId': '6cd8f13c-25a3-48a1-8497-6fc9786c04af',\n",
      " 'lastModified': '2021-01-05T19:03:52.277837',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '1h',\n",
      " 'workflowUri': 's3://workflow/4d86ebfa11294c0799a5ece24b0a164c/workflows.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "response_vgg16_train = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "        \"-P batch_size=20\",\n",
    "        \"-P register_model=True\",\n",
    "        \"-P model_architecture=vgg16\",\n",
    "        \"-P epochs=30\",\n",
    "        \"-P data_dir_train=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Training\",\n",
    "        \"-P data_dir_test=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Test\",\n",
    "    ]),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    timeout=\"1h\",\n",
    ")\n",
    "\n",
    "print(\"Training job for VGG16 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_vgg16_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Adversarial Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our trained model, next we will apply the adversarial patch attack on the network to generate adversarial images.\n",
    "Then, after we have the adversarial images, we will use them to evaluate some standard machine learning metrics against both models.\n",
    "This will give us a sense of the transferability of the attacks between models.\n",
    "\n",
    "This specific workflow is an example of jobs that contain dependencies, as the metric evaluation jobs cannot start until the adversarial image generation jobs have completed.\n",
    "The lab architecture allows users to declare one-to-many job dependencies like this, which we will use to queue up jobs to start immediately after the previous jobs have concluded.\n",
    "The code below illustrates this by doing the following:\n",
    "\n",
    "1. A job is submitted that generates adversarial images based on the VGG16 architecture.\n",
    "1. We wait until the job starts and a MLFlow identifier is assigned, which we check by polling the API until we see the id appear.\n",
    "1. Once we have an id returned to us from the API, we queue up the metrics evaluation jobs and declare the job dependency using the `depends_on` option.\n",
    "1. The message \"Dependent jobs submitted\" will display once everything is queued up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch attack (VGG16 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-01-05T19:03:52.277837',\n",
      " 'dependsOn': None,\n",
      " 'entryPoint': 'train',\n",
      " 'entryPointKwargs': '-P batch_size=20 -P register_model=True -P '\n",
      "                     'model_architecture=vgg16 -P epochs=30 -P '\n",
      "                     'data_dir_train=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Training '\n",
      "                     '-P '\n",
      "                     'data_dir_test=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Test',\n",
      " 'experimentId': 11,\n",
      " 'jobId': '6cd8f13c-25a3-48a1-8497-6fc9786c04af',\n",
      " 'lastModified': '2021-01-05T19:03:56.324629',\n",
      " 'mlflowRunId': '75de83db06634b4f98e9409f17fc28db',\n",
      " 'queueId': 2,\n",
      " 'status': 'started',\n",
      " 'timeout': '1h',\n",
      " 'workflowUri': 's3://workflow/4d86ebfa11294c0799a5ece24b0a164c/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mlflow_run_id_is_not_known(response_patch):\n",
    "    return response_patch[\"mlflowRunId\"] is None and response_patch[\"status\"] not in [\n",
    "        \"failed\",\n",
    "        \"finished\",\n",
    "    ]\n",
    "\n",
    "# Get job ID for training job\n",
    "while mlflow_run_id_is_not_known(response_vgg16_train):\n",
    "    time.sleep(1)\n",
    "    response_vgg16_train = restapi_client.get_job_by_id(response_vgg16_train['jobId'])\n",
    "\n",
    "# Create Patches\n",
    "response_vgg16_patches = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"gen_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P model={EXPERIMENT_NAME}_vgg16/1\",\n",
    "            \"-P model_architecture=vgg16\",\n",
    "            \"-P data_dir=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Training\",\n",
    "            \"-P num_patch_gen_samples=10\",\n",
    "            \"-P num_patch=1\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_vgg16_train[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch attack (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_vgg16_patches)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can poll the status of the dependent jobs using the code below.\n",
    "We should see the status of the jobs shift from \"queued\" to \"started\" and eventually become \"finished\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying and Testing Adversarial Patches\n",
    "\n",
    "Now we will apply the adversarial patches over our test set and evaluate the performance of the baseline model on the adversarial patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch deployment (VGG16 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-01-06T07:43:22.446223',\n",
      " 'dependsOn': 'ceed57ae-3372-49ac-aa82-5db2f20c0304',\n",
      " 'entryPoint': 'deploy_patch',\n",
      " 'entryPointKwargs': '-P run_id=80fee92e26994f8994c0e78d70d2d946 -P '\n",
      "                     'model=howard_fruits360_adversarial_patches_vgg16/1 -P '\n",
      "                     'model_architecture=vgg16 -P '\n",
      "                     'data_dir=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Training',\n",
      " 'experimentId': 11,\n",
      " 'jobId': '7c9d0bb9-779a-461f-a257-e22e5a2c823f',\n",
      " 'lastModified': '2021-01-06T07:43:22.446223',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/212c5e01b31241b59218c8c7c76b8b2f/workflows.tar.gz'}\n",
      "\n",
      "Patch deployment (VGG16 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-01-06T07:43:26.555919',\n",
      " 'dependsOn': 'ceed57ae-3372-49ac-aa82-5db2f20c0304',\n",
      " 'entryPoint': 'deploy_patch',\n",
      " 'entryPointKwargs': '-P run_id=80fee92e26994f8994c0e78d70d2d946 -P '\n",
      "                     'model=howard_fruits360_adversarial_patches_vgg16/1 -P '\n",
      "                     'model_architecture=vgg16 -P '\n",
      "                     'data_dir=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Test',\n",
      " 'experimentId': 11,\n",
      " 'jobId': 'c6f95f33-5698-4fab-9431-54aa0131cf4f',\n",
      " 'lastModified': '2021-01-06T07:43:26.555919',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/5fd5e528c0f34a27aeb16c502ea8bc9f/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wait for Patch attack to finish.\n",
    "while mlflow_run_id_is_not_known(response_vgg16_patches):\n",
    "    time.sleep(1)\n",
    "    response_vgg16_patches = restapi_client.get_job_by_id(response_vgg16_patches['jobId'])\n",
    "\n",
    "\n",
    "# Deploy Patch attack on training set.\n",
    "response_deploy_vgg16_patches_training = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"deploy_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_vgg16_patches['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_vgg16/1\",\n",
    "            \"-P model_architecture=vgg16\",\n",
    "            \"-P data_dir=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Training\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_vgg16_patches[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch deployment (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_deploy_vgg16_patches_training)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "while mlflow_run_id_is_not_known(response_deploy_vgg16_patches_training):\n",
    "    time.sleep(1)\n",
    "    response_deploy_vgg16_patches_training = restapi_client.get_job_by_id(response_deploy_vgg16_patches_training[\"jobId\"])\n",
    "\n",
    "    \n",
    "# Deploy Patch attack on test set.\n",
    "response_deploy_vgg16_patches_testing = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"deploy_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_vgg16_patches['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_vgg16/1\",\n",
    "            \"-P model_architecture=vgg16\",\n",
    "            \"-P data_dir=/nfs/data/Fruits360-Kaggle-2019/fruits-360/Test\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_vgg16_patches[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch deployment (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_deploy_vgg16_patches_testing)\n",
    "print(\"\")\n",
    "\n",
    "# Deploy Patches\n",
    "while mlflow_run_id_is_not_known(response_deploy_vgg16_patches_testing):\n",
    "    time.sleep(1)\n",
    "    response_deploy_vgg16_patches_testing = restapi_client.get_job_by_id(response_deploy_vgg16_patches_testing[\"jobId\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch evaluation (VGG16 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-01-06T07:43:41.731729',\n",
      " 'dependsOn': 'c6f95f33-5698-4fab-9431-54aa0131cf4f',\n",
      " 'entryPoint': 'infer',\n",
      " 'entryPointKwargs': '-P run_id=fcdd79ffac914956b84c4c2bde6aa067 -P '\n",
      "                     'model=howard_fruits360_adversarial_patches_vgg16/1 -P '\n",
      "                     'batch_size=512 -P model_architecture=vgg16 -P '\n",
      "                     'dataset_tar_name=adversarial_patch_dataset.tar.gz -P '\n",
      "                     'dataset_name=adv_patch_dataset',\n",
      " 'experimentId': 11,\n",
      " 'jobId': '1a39d698-9be1-430a-9032-33c19b3c0343',\n",
      " 'lastModified': '2021-01-06T07:43:41.731729',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/99ed3410c94c446980668cc2be4906b1/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check patched dataset results   \n",
    "\n",
    "response_infer_vgg16_patch = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_deploy_vgg16_patches_testing['mlflowRunId']}\",\n",
    "            f\"-P model={EXPERIMENT_NAME}_vgg16/1\",\n",
    "            \"-P batch_size=512\",\n",
    "            \"-P model_architecture=vgg16\",\n",
    "            \"-P dataset_tar_name=adversarial_patch_dataset.tar.gz\",\n",
    "            \"-P dataset_name=adv_patch_dataset\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_vgg16_patches_testing[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch evaluation (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_infer_vgg16_patch)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Training Defense\n",
    "\n",
    "Finally, we will train a new copy of the VGG16 model on training set that contains adversarial patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch adversarial training (VGG16 architecture) job submitted\n",
      "\n",
      "{'createdOn': '2021-01-06T09:14:31.382495',\n",
      " 'dependsOn': '7c9d0bb9-779a-461f-a257-e22e5a2c823f',\n",
      " 'entryPoint': 'train',\n",
      " 'entryPointKwargs': '-P '\n",
      "                     'testing_dataset_run_id=fcdd79ffac914956b84c4c2bde6aa067 '\n",
      "                     '-P '\n",
      "                     'training_dataset_run_id=af0147a846dc4882b1f13ff4510813f5 '\n",
      "                     '-P batch_size=256 -P register_model=True -P '\n",
      "                     'model_architecture=vgg16 -P model_tag=adversarial_patch '\n",
      "                     '-P epochs=10 -P data_dir_train=/nfs/data/Mnist/training '\n",
      "                     '-P data_dir_test=/nfs/data/Mnist/testing -P '\n",
      "                     'load_dataset_from_mlruns=True',\n",
      " 'experimentId': 11,\n",
      " 'jobId': '92b6c71f-f25f-4192-b8f3-aedddce29af7',\n",
      " 'lastModified': '2021-01-06T09:14:31.382495',\n",
      " 'mlflowRunId': None,\n",
      " 'queueId': 2,\n",
      " 'status': 'queued',\n",
      " 'timeout': '24h',\n",
      " 'workflowUri': 's3://workflow/66c95e849bd94645bcfbe450c0f5663e/workflows.tar.gz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_patches_adv_training = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P testing_dataset_run_id={response_deploy_vgg16_patches_testing['mlflowRunId']}\",\n",
    "            f\"-P training_dataset_run_id={response_deploy_vgg16_patches_training['mlflowRunId']}\",\n",
    "            \"-P batch_size=256\",\n",
    "            \"-P register_model=True\",\n",
    "            \"-P model_architecture=vgg16\",\n",
    "            \"-P model_tag=adversarial_patch\",\n",
    "            \"-P epochs=10\",\n",
    "            \"-P data_dir_train=/nfs/data/Mnist/training\",\n",
    "            \"-P data_dir_test=/nfs/data/Mnist/testing\",\n",
    "            \"-P load_dataset_from_mlruns=True\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_vgg16_patches_training[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch adversarial training (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_patches_adv_training)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the MLFlow Tracking Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the lab API can only be used to register experiments and start jobs, so if users wish to extract their results programmatically, they can use the `MlflowClient()` class from the `mlflow` Python package to connect and query their results.\n",
    "Since we captured the run ids generated by MLFlow, we can easily retrieve the data logged about one of our jobs and inspect the results.\n",
    "To start the client, we simply need to run,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client uses the environment variable `MLFLOW_TRACKING_URI` to figure out how to connect to the MLFlow Tracking Service, which we configured near the top of this notebook.\n",
    "To query the results of one of our runs, we just need to pass the run id to the client's `get_run()` method.\n",
    "As an example, let's query the run results for the patch attack applied to the VGG16 architecture,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_adv_patches = mlflow_client.get_run(response_patches_adv_training[\"mlflowRunId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the request completed successfully, we should now be able to query data collected during the run.\n",
    "For example, to review the collected metrics, we just use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_adv_patches.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the run's parameters, we use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_adv_patches.data.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the run's tags, we use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(run_adv_patches.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things you can query using the MLFlow client.\n",
    "[The MLFlow documentation gives a full overview of the methods that are available](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
